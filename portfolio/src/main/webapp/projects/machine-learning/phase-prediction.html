<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project Phase Prediction</title>
    <link rel="stylesheet" type="text/css" href="../../styles/nav.css">
    <link rel="stylesheet" type="text/css" href="../../styles/project-post.css">
    <link rel="stylesheet" type="text/css" href="../../tooltipster-master/dist/css/plugins/tooltipster/sideTip/themes/tooltipster-sideTip-light.min.css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Libre+Franklin:200i&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../../tooltipster-master/dist/css/tooltipster.bundle.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
    <script type="text/javascript" src="../../tooltipster-master/dist/js/tooltipster.bundle.min.js"></script>
    <script>
        $(document).ready(function() {
            $('.tooltip').tooltipster({
                theme: 'tooltipster-light',
                interactive: true,
                maxWidth: 600,
            });
        });
    </script>
</head>
<body>
<div class="top-nav">
    <button class="trigger"><img src="../../img/MenuIcon.png" style="max-height: 100%; max-width: 100%; width: 50%; height: 35%; pointer-events: none;"></button>
    <div class="navbar">
        <a href="../../index.html">R Y L A N  &nbsp; M A R I A N C H U K &nbsp; ></a>
        <a href="../../projects.html">P R O J E C T S</a>
    </div>
</div>
<ul class="side-menu">
    <li class="menu-items"><a href="../../index.html"> H O M E </a></li>
    <li class="menu-items"><a href="../../projects.html">P R O J E C T S</a></li>
    <li class="menu-items"><a href="../../personal2.html">B I O</a></li>
</ul>
<div class="side-nav"></div>

<div class="background-heading rellax" data-rellax-speed="1">
    <div class="heading">P R O J E C T</div>
    <div class="heading">Design of Emulsified Protein Binding System using Classification Algorithms</div>
</div>

<div class="subtitle" style="font-size: 1em;">

    This work contributed to the award of best software project at the International Genetically Engineered Machine, November 4th, 2019.

</div>
<div class="subtitle" style="font-size: 1em;">

    Other version of this work on our UCalgary iGEM team's wiki <b><a href="https://2019.igem.org/Team:Calgary/Model/EmulsionPrediction">here</a></b>.

</div>
<div class="subtitle">

    October, 2019

</div>


<div class="content-page rellax" data-rellax-speed="3">
    <br>
    <br>
    <div class="sub-section">BRIEF</div>
    <hr class="under-subheading">

    <div class="text-block">
        This summer I was selected to work on software and modelling research with the UCalgary iGEM team. Annually, the international
        Genetically Engineered Machine (iGEM) competition brings teams across the world to present projects that apply
        the principles of genetic engineering and synthetic biology to real-world challenges. The themes of the competition
        are open ended, allowing teams to tackle any problem facing a chosen industry. This year, the iGEM Calgary 2019
        team aims to tackle the issue of green seed within the Canadian canola agricultural industry.
        <span class="tooltip highlight" title="Here is our wiki"><u><a href="https://2019.igem.org/Team:Calgary">Read more about our
        solution.</a></u></span>
        <br><br>
        This post will outline my contribution and learning while working with the team.
    </div>


    <div class="sub-section">AIM</div>
    <hr class="under-subheading">

    <div class="text-block">
        Oil and water emulsions play a major role in the success of our project. Once solutions containing oil, surfactant,
        and water are emulsified, they settle out into distinct <span class="tooltip highlight" title="A liquid chemical phase is a distinct homogeneous form of matter with uniform properties."><u>phases</u></span>.
        These specific types of phases depends (in our case) on four variables: the ratios of oil, water, surfactant, and the temperature of the solution.
        <br>
        <br>
        The goal is to find the minimum composition of water needed to sufficiently separate out all chlorophyll. However
        we cannot go through each possible composition of all water, oil, and surfactant. Hence I implemented machine learning
        classification models to generate the phase boundaries which are most statistically probable given sparse training
        data obtained <i>experimentally </i>.
    </div>

    <div class="sub-section">DESCRIPTION OF EQUILIBRIUM PHASES</div>
    <hr class="under-subheading">

    <div class="text-block">
        The type of equilibrium after emulsifying classified by phase using the Windsor scale, depicted below.
        <img src="../../img/phase-prediction/EmulsionClassesnew.PNG" width="90%" height="90%" class="figure">
        <hr>
        <span class="figure-desc">FIGURE 1: The classifications of the various emulsion equilibra, which is a function of the temperature of the emulsion and the concentration ratios of
        oil, water, and surfactant.</span>
        <br><br>
    </div>

    <div class="sub-section">WHY APPLY MACHINE LEARNING?</div>
    <hr class="under-subheading">

    <div class="text-block">
        Phase diagrams are graphs used in chemical and material engineering that map the physical properties of a solution
        as a function of its’ chemical constituents and conditions. Solutions of emulsions can be described using this
        approach, however the diagrams are normally generated through tedious experimentation. There is also no widely
        accepted way to model solutions of emulsions, and the few that do require extremely difficult measurements.
        <br>
        <br>

        For the purposes of rapid design, we are more interested in correlating the chemical components to it’s Winsor
        type rather than having a thorough understanding as to why the system behaves the way it does. In a nutshell,
        Machine Learning excels at identifying correlation and can provide an abstract reading without understanding the
        underlying thermodynamic molecular details, which makes it the perfect method for us.
        <br>
        <br>

        It is common practice to display phases as a function of the solution composition ratio using a ternary diagram
        (a readable way to present 4-dimensional data). They look this this:
        <iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://plot.ly/~ryonix/1.embed" height="825" width="100%"></iframe>
        <hr>
        <span class="figure-desc">FIGURE 2: An example of the data collected <i>experimentally</i> that is to be classified with machine learning approaches.</span>
        <br><br>
        This is analogous to the data that will be collected in the lab, however the colors here do not correspond to a phase yet.
        <br>
        <br>
        Our task is to then approximate the <span class="tooltip highlight" title="Finding the color division"><u>phase boundaries</u></span> by statistical approximations.
        <b>I implement and compare \(\mathcal{K}\)-Nearest Neighbours and Support Vector Classification. From there, I generated a confidence function which
        gives a normalized value representing how much the classified point changes when iterating over Support Vector Classification parameters.</b>
    </div>

    <div class="sub-section">OUTLINE OF DATA</div>
    <hr class="under-subheading">
    <div class="text-block">
        The data given is four dimensional, containing three compositions of oil, water, and surfactant, and its <span class="tooltip highlight" title="Classification label"><u>equilibrium
        phase.</u></span> Our model is looking to find a function \(F \; : \; \vec{v} \longrightarrow y \> \) mapping a given vector to a phase class such that,
        $$ \vec{v} = \begin{pmatrix} r_{oil} \\ r_{water} \\ r_{surfct} \end{pmatrix}$$
        $$ r_{oil} + r_{water} + r_{surfct} = 1$$
        $$ y \in \{Windsor1, Windsor2, Windsor3, Windsor4\}$$
    </div>

    <div class="sub-section">\(\mathcal{K}\)-NEAREST NEIGHBOURS CLASSIFICATION</div>
    <hr class="under-subheading">
    <div class="text-block">
        The aim of a general classification model is to provide the likelihood a new unlabelled vector lies within a class.
        The \(\mathcal{K}\)-Nearest Neighbours method is a non-parametric approach which looks at the \(\mathcal{K}\) nearest (in terms of distance)
        vectors within the space and assigns a label based on those closest neighbours. The probability given a vector
        from described above will be labeled with phase can be calculated with KNN by:
        $$ Pr( \> Y = y \> | \> X = \vec{v}) = \frac{1}{\mathcal{K}} \sum_{i \in \mathcal{N}}^{} I(\> y_i = y \>)$$
        Where \(i\) indexes through the \(\mathcal{K}\) nearest vectors in \(\mathcal{N}\) and <i>I</i> is the identity function which outputs a 1 if the label of the neighbour is equal to  and 0 otherwise (Hastie et al. 2017).
    </div>

    <div class="sub-section">SUPPORT VECTOR CLASSIFICATION</div>
    <hr class="under-subheading">
    <div class="text-block">
        Support Vector Classification (SVC) provides a classification approach which finds a hyperplane that divides two
        classes of vectors within a space. The goal is to find the maximum margin between the labelled data and generate
        parameters for a hyperplane that would divide this margin. The optimization problem of generating a separating
        hyperplane between two classes holding \(n\) data points can be summarized:
        $$ \max_{\beta_0, \beta_1, \beta_2, \beta_3, \epsilon_i, \ldots, \epsilon_n} \mathcal{M} $$
        subject to,
        $$\beta_0^2 + \beta_1^2 + \beta_2^2 + \beta_3^2 =1 $$
        $$ y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} \geq \mathcal{M}(1-\epsilon_i)$$
        $$ \sum\limits_{i=0}^{n} \epsilon_i \leq \mathcal{C}, \>\>\>\>\> \epsilon \geq 0, \>\>\>\>\> y_i \in \{1, -1\}.$$
        Where \(\mathcal{M}\) is the size of the margin, \(\beta_i\) are the parameters defining the hyperplane, \(y_i\) is the label of each vector which
        can only be 1 or -1. \(\epsilon_i\) is the error for each vector which is constrained by \(\mathcal{C}\), the cost parameter (Hastie et al. 2017).
        <br>
        Since we have four classes to be separated, we applied the one-versus-one approach, where divisions were
        constructed for each pair of classes, meaning this optimization was solved 6 times (the number of distinct pairs between 4 classes).
        Since the data is not linearly separable, a non-linear radial basis function (RBF) was used as a kernel:
        $$K(\vec{v_0}, \vec{v_i}) = e^{- \; \gamma \; \vec{v_0} \; \dot \; \vec{v_i}}$$
        Where \(\vec{v_0}\) is the vector to be labelled, and the kernel is applied on each training vector \(\vec{v_i}\) for this test observation.
        \( \gamma \) is a parameter subject to choice.
        <br>
        The second parameter \(\mathcal{C}\) specifies the amount of errors allowed within the separating hyperplane,
        allowing the adjustment of the model’s bias-variance trade off. This trade off is an important consideration in
        the approximation of any function. Approximations that are more flexible have greater variance (tend to follow
        the data closely) and have low bias. A low value of \(\mathcal{C}\) means the separation cannot allow for many errors, which
        implies the model will look more flexible and possibly overfit.
    </div>
    <div class="sub-section">K-FOLD CROSS VALIDATION</div>
    <hr class="under-subheading">
    <div class="text-block">
        \(K\)-fold cross validation provides some reasoning for the parameter choices within the model. To attempt to validate
        the model’s classifications, the data is split into two partitions: train and test. \(K\)-fold cross validation
        outlines the way to choose such partitions. The model is trained and fed the test observations without a label.
        The model predicts each unlabelled test observation and is compared to its actual label (class), giving a
        quantification on the models accuracy. For \(K\) partitions of the data, the mean error rate of a model with given
        parameters is calculated as:
        $$ \frac{1}{N}\sum\limits_{i=1}^{K} \sum\limits_{y_p \in Y_i}^{} I(\> y_p \neq y_a ) $$

        Where \(N\) is the number of data points (including train and test), \(y_p\) is the predicted label calculated by the model for all test vectors in partition \(Y_i\) , and \(y_a\) is the actual label of the vector (Hastie et al. 2017).
        It is not sufficient to merely choose a single subset for testing because it can be an inaccurate representation
        of which points the model can predict well. \(K\)-fold Cross Validation attempts to address this by using \(K\) partitions
        of the data to resample. The error rate is then determined by averaging each of the partition error, providing a
        more representative reading of the model’s true error rate.
    </div>
    <div class="sub-section">COMPARISON OF \(\mathcal{K}\)-NEAREST NEIGHBOURS AND SUPPORT VECTOR CLASSIFICATION</div>
    <hr class="under-subheading">
    <div class="text-block">
        I implemented 10-fold cross validation on a range of parameter combinations with both SVC and KNN and chose the model
        with the lowest error rate. KNN performed significantly worse than SVC. The table below lists the optimal parameters for only
        the Support Vector Classification model over four temperatures.
        <br>
        <br>
        <table class="table" style="text-align: center;">
            <tr>
                <th>Temperature</th>
                <th>\(\gamma\)</th>
                <th>COST</th>
                <th>CV Error Rate</th>
            </tr>
            <tr>
                <td>300K</td>
                <td>0.66</td>
                <td>18400</td>
                <td>0.2428</td>
            </tr>
            <tr>
                <td>310K</td>
                <td>0.25</td>
                <td>12200</td>
                <td>0.1714</td>
            </tr>
            <tr>
                <td>323K</td>
                <td>0.75</td>
                <td>8800</td>
                <td>0.1571</td>
            </tr>
            <tr>
                <td>343K</td>
                <td>0.17</td>
                <td>10600</td>
                <td>0.2428</td>
            </tr>
        </table>
        <br>
        <br>
        <img src="../../img/phase-prediction/GeneratedDiagSVM.PNG" width="90%" height="90%" class="figure">
        <hr>
        <span class="figure-desc">FIGURE 3: A completed phase diagram, displaying the phase change at different compositions of oil, water and surfactant.
        Gamma: 0.66, Cost: 18400, Error Rate: 0.2428. (Dark blue: Winsor 1, grey: Winsor 2, green: Winsor 3, light blue: Winsor 4)</span>
    </div>

    <div class="sub-section">CLASSIFICATION CONFIDENCE</div>
    <hr class="under-subheading">
    <div class="text-block">
        I wondered which points are difficult for the SVM to classify? Those areas on the ternary diagram which have multiple
        classes do not have straightforward division by a hyperplane. How do we identify these areas?
        <br>
        <br>
        I generated 180 SVC models with different parameter combinations going from underfit to overfit, and found those points
        that changed the most over these parameter changes. High number of changes implied a low 'confidence' in classification
        of that point. The confidence function depended on the number of changes in the phase label with increased overfitting.
        <br>
        <br>
        The figure below visually displays my approach to identifying areas of low classification confidence.

        The lab can choose how to construct their emulsions by choosing to construct an emulsion identified in areas of
        high classification confidence.
        <img src="../../img/phase-prediction/ConfidenceVisual.svg" width="90%" height="90%" class="figure">
        <hr>
        <span class="figure-desc">FIGURE 3:  The approach at finding a confidence value for each emulsion point. Darker
            areas of the color gradient show those points that were difficult to classify. Yellow regions show no phase
            change over all Support Vector Classification parameter changes.
        </span>
    </div>

    <div class="sub-section">PHASE CHANGE OVER TEMPERATURE</div>
    <hr class="under-subheading">
    <div class="text-block">
        Emulsions have different chemical properties with changes in temperature. I used a linear transformation to reduce
        the ternary plot into two dimensions to allow multiple phase diagrams to be viewed in 3-Dimensional space
        with temperature as a third value. The GIF below depicts the phase change over temperature in 3-D space.
    </div>
    <img src="../../img/phase-prediction/ternaryPhaseDiagram.gif" width="60%" height="60%" class="figure">


    <div class="sub-section">CONCLUSION</div>
    <hr class="under-subheading">
    <div class="text-block">
        This model provides knowledge on emulsion formulation, which is an indispensable part of the UCalgary 2019 iGEM
        project, allowing for viable refinement of canola oil by dispensing chlorophyll that would be detrimental to its
        production. It is desirable to find the correct ratios of the emulsion constituents that lead to a Winsor 1 type
        equilibrium because the pure oil phase is left without a microemulsion. Determining these phase equilibria
        boundaries is therefore an imminent task which we provided a solution with the application of \(\mathcal{K}\)-Nearest Neighbours
        and Support Vector Classification, both machine learning classification models. With merely sparse data samples
        gathered in vitro, Support Vector Classification predicted phase boundaries at its best with an error rate of 17%
        which could be improved with more samples taken in regions of low classification confidence. Hence, this method
        provides a practical alternative to other deterministic methods which require rigorous effort and time.
        <br><br>
        We have patented this work and it was recognized to win numerous categories at the International Genetically Engineered Machine competition.
        Out of around 170 undergraduate teams, this work won best software and a nomination for best model.
        See our wiki <b><a href="https://2019.igem.org/Team:Calgary/Model/EmulsionPrediction">here</a></b>.
    </div>
    <div class="sub-section">REFERENCES</div>
    <hr class="under-subheading">
    <div class="text-block">
        Hastie, T., Friedman, J., & Tisbshirani, R. (2017). The Elements of statistical learning: Data mining, inference, and prediction. Springer.
    </div>

</div>


<hr>
<div class="footer"></div>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<script src="../../rellax.min.js"></script>
<script src="../../script.js"></script>
</body>
</html>